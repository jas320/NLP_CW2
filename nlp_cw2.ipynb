{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "from urllib import request\n",
    "import random\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare logger\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# # check gpu\n",
    "# cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# print('Cuda available? ',cuda_available)\n",
    "# if cuda_available:\n",
    "#   import tensorflow as tf\n",
    "#   # Get the GPU device name.\n",
    "#   device_name = tf.test.gpu_device_name()\n",
    "#   # The device name should look like the following:\n",
    "#   if device_name == '/device:GPU:0':\n",
    "#       print('Found GPU at: {}'.format(device_name))\n",
    "#   else:\n",
    "#       raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
     ]
    }
   ],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))\n",
    "\n",
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of label to numerical label:\n",
      "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
     ]
    }
   ],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "dpm = DontPatronizeMe('.', '.')\n",
    "dpm.load_task1()\n",
    "dpm.load_task2(return_one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "teids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "trids.par_id = trids.par_id.astype(str)\n",
    "teids.par_id = teids.par_id.astype(str)\n",
    "data=dpm.train_task1_df\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = [] # will contain par_id, label and text\n",
    "for idx in range(len(trids)):  \n",
    "  parid = trids.par_id[idx]\n",
    "  #print(parid)\n",
    "  # select row from original dataset to retrieve `text` and binary label\n",
    "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "  text = data.loc[data.par_id == parid].text.values[0]\n",
    "  label = data.loc[data.par_id == parid].label.values[0]\n",
    "  item = {\n",
    "      'par_id':parid,\n",
    "      'community':keyword,\n",
    "      'text':text,\n",
    "      'label':label\n",
    "  }\n",
    "  rows.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136</td>\n",
       "      <td>homeless</td>\n",
       "      <td>Durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10352</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8279</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>Far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1164</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>To strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>8380</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Rescue teams search for survivors on the rubbl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>8381</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>The launch of ' Happy Birthday ' took place la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>8382</td>\n",
       "      <td>homeless</td>\n",
       "      <td>The unrest has left at least 20,000 people dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>8383</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>You have to see it from my perspective . I may...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>8384</td>\n",
       "      <td>disabled</td>\n",
       "      <td>Yet there was one occasion when we went to the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id      community                                               text  \\\n",
       "0      4341  poor-families  The scheme saw an estimated 150,000 children f...   \n",
       "1      4136       homeless  Durban 's homeless communities reconciliation ...   \n",
       "2     10352  poor-families  The next immediate problem that cropped up was...   \n",
       "3      8279     vulnerable  Far more important than the implications for t...   \n",
       "4      1164  poor-families  To strengthen child-sensitive social protectio...   \n",
       "...     ...            ...                                                ...   \n",
       "8370   8380        refugee  Rescue teams search for survivors on the rubbl...   \n",
       "8371   8381       hopeless  The launch of ' Happy Birthday ' took place la...   \n",
       "8372   8382       homeless  The unrest has left at least 20,000 people dea...   \n",
       "8373   8383       hopeless  You have to see it from my perspective . I may...   \n",
       "8374   8384       disabled  Yet there was one occasion when we went to the...   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "8370      0  \n",
       "8371      0  \n",
       "8372      0  \n",
       "8373      0  \n",
       "8374      0  \n",
       "\n",
       "[8375 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf1 = pd.DataFrame(rows)\n",
    "trdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = [] # will contain par_id, label and text\n",
    "for idx in range(len(teids)):  \n",
    "  parid = teids.par_id[idx]\n",
    "  #print(parid)\n",
    "  # select row from original dataset\n",
    "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "  text = data.loc[data.par_id == parid].text.values[0]\n",
    "  label = data.loc[data.par_id == parid].label.values[0]\n",
    "  test_rows.append({\n",
    "      'par_id':parid,\n",
    "      'community':keyword,\n",
    "      'text':text,\n",
    "      'label':label\n",
    "  })\n",
    "  #TODO: keyword?Country?length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>in-need</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>homeless</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10462</td>\n",
       "      <td>homeless</td>\n",
       "      <td>The sad spectacle , which occurred on Saturday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>10463</td>\n",
       "      <td>refugee</td>\n",
       "      <td>\"\"\" The Pakistani police came to our house and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>10464</td>\n",
       "      <td>disabled</td>\n",
       "      <td>\"When Marie O'Donoghue went looking for a spec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10465</td>\n",
       "      <td>women</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>10466</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id   community                                               text  \\\n",
       "0      4046    hopeless  We also know that they can benefit by receivin...   \n",
       "1      1279     refugee  Pope Francis washed and kissed the feet of Mus...   \n",
       "2      8330     refugee  Many refugees do n't want to be resettled anyw...   \n",
       "3      4063     in-need  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...   \n",
       "4      4089    homeless  \"In a 90-degree view of his constituency , one...   \n",
       "...     ...         ...                                                ...   \n",
       "2089  10462    homeless  The sad spectacle , which occurred on Saturday...   \n",
       "2090  10463     refugee  \"\"\" The Pakistani police came to our house and...   \n",
       "2091  10464    disabled  \"When Marie O'Donoghue went looking for a spec...   \n",
       "2092  10465       women  \"Sri Lankan norms and culture inhibit women fr...   \n",
       "2093  10466  vulnerable  He added that the AFP will continue to bank on...   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "2089      0  \n",
       "2090      0  \n",
       "2091      0  \n",
       "2092      0  \n",
       "2093      0  \n",
       "\n",
       "[2094 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tedf1 = pd.DataFrame(test_rows)\n",
    "tedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedf1 = tedf1.sample(frac = 1) # shuffle data\n",
    "# downsample negative instances\n",
    "pcldf = trdf1[trdf1.label==1]\n",
    "npos = len(pcldf)\n",
    "\n",
    "training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])\n",
    "# tedf1.par_id\n",
    "train_data = training_set1\n",
    "test_data = tedf1\n",
    "train_data_text_list = train_data.text.to_list()\n",
    "train_data_label_list = train_data.label.to_list()\n",
    "test_data_text_list = test_data.text.to_list()\n",
    "test_data_label_list = test_data.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def calc_scores_and_print_miscalculations(true_lables, pred_labels, unseen_data, n=10):\n",
    "    '''prints accuracy, f1 score and first N miscalculations'''\n",
    "    calc_scores(true_lables, pred_labels)\n",
    "    # Print predicted labels for unseen test data\n",
    "    print(\"\\nMisclassifications:\")\n",
    "    c = 0\n",
    "    for text, true_label, pred_label in zip(unseen_data, true_lables, pred_labels):\n",
    "        if true_label != pred_label:\n",
    "            print(f\"Pred: {pred_label} Ac: {true_label} Text: {text}\")\n",
    "            c += 1\n",
    "            if c == n:\n",
    "                break\n",
    "\n",
    "def calc_scores(true_lables, pred_labels):\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(true_lables, pred_labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    f1 = f1_score(true_lables, pred_labels)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    return (accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark - Bag of words (no augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Joshua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Joshua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Text 1: person put_down my only miniature\n",
      "Augmented Text 2: somebody destruct my only diddle\n",
      "Augmented Text 3: someone destruct my only play\n"
     ]
    }
   ],
   "source": [
    "# Synonm augmentation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(text, n=3):\n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "    for _ in range(n):\n",
    "        new_words = words.copy()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() not in stop_words:  # Check if the word is not a stop word\n",
    "                synonyms = get_synonyms(word)\n",
    "                if synonyms:\n",
    "                    synonym = random.choice(synonyms)\n",
    "                    new_words[i] = synonym\n",
    "        augmented_texts.append(' '.join(new_words))\n",
    "    return augmented_texts\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"Somebody destroyed my only toy\"\n",
    "augmented_texts = synonym_replacement(original_text, n=3)  # Generate 3 augmented texts\n",
    "for i, augmented_text in enumerate(augmented_texts, 1):\n",
    "    print(f\"Augmented Text {i}: {augmented_text}\")\n",
    "    \n",
    "\n",
    "def replacement_text(text_list, label_list, transform_f=synonym_replacement):\n",
    "    aug_text_list = []\n",
    "    aug_label_list = []\n",
    "    for text, label in zip(text_list, label_list):\n",
    "        options = transform_f(text)\n",
    "        aug_text_list.append(text)\n",
    "        aug_text_list.extend(options)\n",
    "        aug_label_list.append(label)\n",
    "        aug_label_list.extend([label] * len(options))\n",
    "    return aug_text_list, aug_label_list\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Not working fully\n",
    "def cluster_sampling(X_train_bow, y_train):\n",
    "    ''' Uses KMEANS and requires that X_train is alreayd in feature form e.g. Bag Of words or TF-IDF, vectorized form'''\n",
    "    # Step 1: Cluster-based Sampling\n",
    "    num_clusters = 5  # Number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_train_bow)\n",
    "\n",
    "    # Sample size per cluster\n",
    "    sample_size_per_cluster = 100\n",
    "\n",
    "    X_train_sampled = []\n",
    "    y_train_sampled = []\n",
    "\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_indices = (clusters == cluster_id).nonzero()[0]\n",
    "        sampled_indices = np.random.choice(cluster_indices, size=min(sample_size_per_cluster, len(cluster_indices)), replace=False)\n",
    "        X_train_sampled.extend(X_train_bow[sampled_indices])\n",
    "        y_train_sampled.extend([y_train[i] for i in sampled_indices])\n",
    "    return X_train_sampled, y_train_sampled\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "import os\n",
    "import json\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "def translate_group(text_list, label_list):\n",
    "    aug_text_list = []\n",
    "    aug_label_list = []\n",
    "    translator_to = GoogleTranslator(source='en', target=random.choice(['fr']))\n",
    "    translator_from = GoogleTranslator(source='fr', target='en')\n",
    "    for text, label in zip(text_list, label_list):\n",
    "        aug_text_list.append(text)\n",
    "        new_text = [translator_from.translate(translator_to.translate(text))]\n",
    "        aug_text_list.extend(new_text)\n",
    "        \n",
    "        aug_label_list.append(label)\n",
    "        aug_label_list.extend([label] * len(new_text))\n",
    "    return aug_text_list, aug_label_list\n",
    "\n",
    "import pickle\n",
    "def write_list_to_file(list1, filename1):\n",
    "    with open(filename1, 'wb') as file:\n",
    "        pickle.dump(list1, file)\n",
    "def read_list_from_file(filename1):\n",
    "    with open(filename1, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "res1, res2 = translate_group(train_data_text_list , train_data_label_list)\n",
    "write_list_to_file(res1, \"french_translation_text.txt\")\n",
    "write_list_to_file(res2, \"french_translation_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computationally expensive transformations\n",
    "X_aug, y_aug = replacement_text(train_data_text_list, train_data_label_list, synonym_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = train_data_text_list, train_data_label_list\n",
    "X,y = read_list_from_file(\"french_translation_text.txt\"), read_list_from_file(\"french_translation_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "Accuracy: 0.95\n",
      "F1 Score: 0.9508196721311475\n",
      "Accuracy: 0.7125119388729704\n",
      "F1 Score: 0.3127853881278539\n",
      "9528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "# Example dataset (X: text data, y: sentiment labels)\n",
    "print(len(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define stopwords and create CountVectorizer with n-grams and stopwords removal\n",
    "stop_words = list(stop_words)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words)\n",
    "\n",
    "# Create BoW vectors\n",
    "maxv = -float('inf')\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 2: Train a logistic regression classifier on the sampled data\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "calc_scores(y_test, y_pred)\n",
    "# Unseen data\n",
    "unseen_data = test_data_text_list\n",
    "true_labels = test_data_label_list\n",
    "\n",
    "# Vectorize the unseen test data using the same CountVectorizer instance\n",
    "X_unseen_bow = vectorizer.transform(unseen_data)\n",
    "\n",
    "# Predict sentiment labels for the unseen test data\n",
    "y_unseen_pred = classifier.predict(X_unseen_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc, f1 = calc_scores(true_labels, y_unseen_pred)\n",
    "        \n",
    "# calc_scores_and_print_miscalculations(true_labels, y_unseen_pred, unseen_data)\n",
    "print(len(X_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model - SVM - TF-IDF feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9166666666666666\n",
      "F1 Score: 0.9173553719008265\n",
      "Accuracy: 0.6575931232091691\n",
      "F1 Score: 0.2865671641791045\n",
      "\n",
      "Misclassifications:\n",
      "Pred: 1 Ac: 0 Text: Tottenham are still homeless . The club expected to be in their new stadium for their match against Liverpool last Saturday but testing threw up safety issues .\n",
      "Pred: 1 Ac: 0 Text: Children in Ky feel the effects of homelessness <h> HOMELESSNESS is threatening children 's education , a new report has found .\n",
      "Pred: 1 Ac: 0 Text: It 's incumbent upon Kenyans not to fall prey to consuming political gimmickry , especially ahead of the election year . It is imponderable that a government can launch projects that are n't budgeted for , but still insist it is n't faking them . Ruto knows he can sell a fad such as name-calling NASA without anticipating calls to substantiate . But I also think politicians around the presidency are increasingly vulnerable to being fed all manner of social media chit-chat as facts .\n",
      "Pred: 1 Ac: 0 Text: In other words , Waitiki 's now settled squatters are being invited to gamble with their newfound status and face the prospect of homelessness all over again .\n",
      "Pred: 0 Ac: 1 Text: Bus driver Cathal Carroll asks if I 've heard the news this morning . I have n't . Four thousand souls have just been rescued from the waters of the Mediterranean , he says . All of them African refugees . All fleeing hunger and persecution in their native lands . What do I think about that ?\n",
      "Pred: 1 Ac: 0 Text: The Americans with Disabilities Act allows a disabled person to bring a service animal into businesses that serve the public . Businesses can ask a service animal to leave if they believe the animal 's behavior may threaten the health or safety of others .\n",
      "Pred: 1 Ac: 0 Text: 3 Vandalizing ancient Buddhist Monuments , Encroaching on Temple land donated to Buddhist temples by the ancient Kings in the yore for their sustenance and protesting against restoring and rehabilitation of ancient religious monuments in the Northern and Eastern Provinces jointly by Tamil and Muslim politicians and even media men and women and activists of the two ethnic groups like Sulochana Ramiah Mohan and Muslims vandalism against Buddhist places even in the interior of the country , ( for example in Kuragala and Katarantenna in Mawanella area ) .\n",
      "Pred: 1 Ac: 0 Text: Students from poor families have to pass a competitive test to get into Super 30 and then commit themselves to a year of 16-hour daily study routine .\n",
      "Pred: 1 Ac: 0 Text: \"The new immigrants also hit up the vendors because they \"\" did n't deliver what they were paid for \"\" .\"\n",
      "Pred: 1 Ac: 0 Text: An advocate for feminism , he highlighted the team was made up of predominantly young women who spoke a common language to achieve the goal of TedX Maitama .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# X,y = train_data_text_list[:300] + train_data_text_list[-300:], train_data_label_list[:300] +  train_data_label_list[-300:] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict sentiment labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "calc_scores(y_test, y_pred)\n",
    "\n",
    "# Example of unseen test data\n",
    "unseen_data = test_data_text_list\n",
    "true_labels = test_data_label_list # True sentiment labels for the unseen test data\n",
    "\n",
    "# Convert the unseen test data into TF-IDF vectors\n",
    "X_unseen_tfidf = tfidf_vectorizer.transform(unseen_data)\n",
    "\n",
    "# Predict sentiment labels for the unseen test data\n",
    "y_unseen_pred = svm_classifier.predict(X_unseen_tfidf)\n",
    "\n",
    "# Calculate accuracy for unseen test data\n",
    "calc_scores_and_print_miscalculations(true_labels, y_unseen_pred, unseen_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
