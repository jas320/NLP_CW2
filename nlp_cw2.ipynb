{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "from urllib import request\n",
    "import random\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available?  False\n"
     ]
    }
   ],
   "source": [
    "# # prepare logger\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# # check gpu\n",
    "# cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# print('Cuda available? ',cuda_available)\n",
    "# if cuda_available:\n",
    "#   import tensorflow as tf\n",
    "#   # Get the GPU device name.\n",
    "#   device_name = tf.test.gpu_device_name()\n",
    "#   # The device name should look like the following:\n",
    "#   if device_name == '/device:GPU:0':\n",
    "#       print('Found GPU at: {}'.format(device_name))\n",
    "#   else:\n",
    "#       raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
     ]
    }
   ],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))\n",
    "\n",
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of label to numerical label:\n",
      "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
     ]
    }
   ],
   "source": [
    "from dont_patronize_me import DontPatronizeMe\n",
    "dpm = DontPatronizeMe('.', '.')\n",
    "dpm.load_task1()\n",
    "dpm.load_task2(return_one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "teids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "trids.par_id = trids.par_id.astype(str)\n",
    "teids.par_id = teids.par_id.astype(str)\n",
    "data=dpm.train_task1_df\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = [] # will contain par_id, label and text\n",
    "for idx in range(len(trids)):  \n",
    "  parid = trids.par_id[idx]\n",
    "  #print(parid)\n",
    "  # select row from original dataset to retrieve `text` and binary label\n",
    "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "  text = data.loc[data.par_id == parid].text.values[0]\n",
    "  label = data.loc[data.par_id == parid].label.values[0]\n",
    "  item = {\n",
    "      'par_id':parid,\n",
    "      'community':keyword,\n",
    "      'text':text,\n",
    "      'label':label\n",
    "  }\n",
    "  rows.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136</td>\n",
       "      <td>homeless</td>\n",
       "      <td>Durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10352</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>The next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8279</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>Far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1164</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>To strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>8380</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Rescue teams search for survivors on the rubbl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>8381</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>The launch of ' Happy Birthday ' took place la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>8382</td>\n",
       "      <td>homeless</td>\n",
       "      <td>The unrest has left at least 20,000 people dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>8383</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>You have to see it from my perspective . I may...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>8384</td>\n",
       "      <td>disabled</td>\n",
       "      <td>Yet there was one occasion when we went to the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id      community                                               text  \\\n",
       "0      4341  poor-families  The scheme saw an estimated 150,000 children f...   \n",
       "1      4136       homeless  Durban 's homeless communities reconciliation ...   \n",
       "2     10352  poor-families  The next immediate problem that cropped up was...   \n",
       "3      8279     vulnerable  Far more important than the implications for t...   \n",
       "4      1164  poor-families  To strengthen child-sensitive social protectio...   \n",
       "...     ...            ...                                                ...   \n",
       "8370   8380        refugee  Rescue teams search for survivors on the rubbl...   \n",
       "8371   8381       hopeless  The launch of ' Happy Birthday ' took place la...   \n",
       "8372   8382       homeless  The unrest has left at least 20,000 people dea...   \n",
       "8373   8383       hopeless  You have to see it from my perspective . I may...   \n",
       "8374   8384       disabled  Yet there was one occasion when we went to the...   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "8370      0  \n",
       "8371      0  \n",
       "8372      0  \n",
       "8373      0  \n",
       "8374      0  \n",
       "\n",
       "[8375 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf1 = pd.DataFrame(rows)\n",
    "trdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = [] # will contain par_id, label and text\n",
    "for idx in range(len(teids)):  \n",
    "  parid = teids.par_id[idx]\n",
    "  #print(parid)\n",
    "  # select row from original dataset\n",
    "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "  text = data.loc[data.par_id == parid].text.values[0]\n",
    "  label = data.loc[data.par_id == parid].label.values[0]\n",
    "  test_rows.append({\n",
    "      'par_id':parid,\n",
    "      'community':keyword,\n",
    "      'text':text,\n",
    "      'label':label\n",
    "  })\n",
    "  #TODO: keyword?Country?length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>refugee</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>in-need</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>homeless</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10462</td>\n",
       "      <td>homeless</td>\n",
       "      <td>The sad spectacle , which occurred on Saturday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>10463</td>\n",
       "      <td>refugee</td>\n",
       "      <td>\"\"\" The Pakistani police came to our house and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>10464</td>\n",
       "      <td>disabled</td>\n",
       "      <td>\"When Marie O'Donoghue went looking for a spec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10465</td>\n",
       "      <td>women</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>10466</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id   community                                               text  \\\n",
       "0      4046    hopeless  We also know that they can benefit by receivin...   \n",
       "1      1279     refugee  Pope Francis washed and kissed the feet of Mus...   \n",
       "2      8330     refugee  Many refugees do n't want to be resettled anyw...   \n",
       "3      4063     in-need  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...   \n",
       "4      4089    homeless  \"In a 90-degree view of his constituency , one...   \n",
       "...     ...         ...                                                ...   \n",
       "2089  10462    homeless  The sad spectacle , which occurred on Saturday...   \n",
       "2090  10463     refugee  \"\"\" The Pakistani police came to our house and...   \n",
       "2091  10464    disabled  \"When Marie O'Donoghue went looking for a spec...   \n",
       "2092  10465       women  \"Sri Lankan norms and culture inhibit women fr...   \n",
       "2093  10466  vulnerable  He added that the AFP will continue to bank on...   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "2089      0  \n",
       "2090      0  \n",
       "2091      0  \n",
       "2092      0  \n",
       "2093      0  \n",
       "\n",
       "[2094 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tedf1 = pd.DataFrame(test_rows)\n",
    "tedf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedf1 = tedf1.sample(frac = 1) # shuffle data\n",
    "# downsample negative instances\n",
    "pcldf = trdf1[trdf1.label==1]\n",
    "npos = len(pcldf)\n",
    "\n",
    "training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])\n",
    "# tedf1.par_id\n",
    "train_data = training_set1\n",
    "test_data = tedf1\n",
    "train_data_text_list = train_data.text.to_list()\n",
    "train_data_label_list = train_data.label.to_list()\n",
    "test_data_text_list = test_data.text.to_list()\n",
    "test_data_label_list = test_data.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def calc_scores_and_print_miscalculations(true_lables, pred_labels, unseen_data, n=10):\n",
    "    '''prints accuracy, f1 score and first N miscalculations'''\n",
    "    calc_scores(true_lables, pred_labels)\n",
    "    # Print predicted labels for unseen test data\n",
    "    print(\"\\nMisclassifications:\")\n",
    "    c = 0\n",
    "    for text, true_label, pred_label in zip(unseen_data, true_lables, pred_labels):\n",
    "        if true_label != pred_label:\n",
    "            print(f\"Pred: {pred_label} Ac: {true_label} Text: {text}\")\n",
    "            c += 1\n",
    "            if c == n:\n",
    "                break\n",
    "\n",
    "def calc_scores(true_lables, pred_labels):\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(true_lables, pred_labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    f1 = f1_score(true_lables, pred_labels)\n",
    "    print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark - Bag of words (no augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.906865671641791\n",
      "F1 Score: 0.3217391304347826\n",
      "Accuracy: 0.894937917860554\n",
      "F1 Score: 0.20863309352517986\n",
      "\n",
      "Misclassifications:\n",
      "Pred: 0 Ac: 1 Text: Speaking in Garissa on Sunday when he hosted officials from Qatar Charity who are financing a programme to assist two orphanages in the county , Korane said vulnerable children stand equal chances of becoming useful members of society if given good upbringing and education .\n",
      "Pred: 0 Ac: 1 Text: 18 December should serve as a time when we look with compassion at the fate of migrants , refugees and the internally displaced . It is especially a time when we must plan and increase resources for creative action .\n",
      "Pred: 0 Ac: 1 Text: Fasting bridges the gap between rich and poor , sustained and impoverished , fulfilled and needy . This experience should then inspire compassion and mercy , which is manifest by generosity of wealth and time to help those in need .\n",
      "Pred: 0 Ac: 1 Text: The World Health Organization did not give a reason for the increase in deaths , but a provincial health official in Sindh said that the disease hit areas where poor families did not vaccinate their children .\n",
      "Pred: 1 Ac: 0 Text: \"He said : \"\" In the same way that we must never countenance the kind of savagery being meted out to our women and children in recent times , we must never get to the point where we accept incidents of multiple murders as a new norm . As we extend sympathies to the families of the teacher in Clarendon and the four men in St Elizabeth , we must resolve as law abiding citizens , to take the fight to the criminals in our midst and combat the terror being unleashed by them on the innocent . \"\" <h> ADVERTISEMENT\"\n",
      "Pred: 1 Ac: 0 Text: From Lagos to Nasarawa , Kebbi , to Warri , the resultant floods which accompany annual rainfall have continued to destroy homes and valuable property . As a result of this development , thousands of Nigerians are now being rendered homeless , many roads are becoming impassable and lives are being lost . Yet our worry stems from the fact that as bad as the situation may seem today , the worst is not over .\n",
      "Pred: 0 Ac: 1 Text: He said the victims who are currently rendered homeless can now be relieved of troubles as the 5,000 iron sheets from Mwanza had arrived , with 1,200 already distributed to victims in Bukoba Municipality .\n",
      "Pred: 0 Ac: 1 Text: When some people feel causing problem for some others by breaking into their homes to steal is n't too good , they just result to begging . You now see people without deformities begging , when some people who are disabled work to feed their mouth . You then ask , what type of country is Nigeria ? Even a man who is not lettered would chorus the maxim that two wrongs do n't make a right . The country is n't working out ; and people want to survive anyhow too . They have to eat they will say .\n",
      "Pred: 0 Ac: 1 Text: But if the Supreme Court gives a favorable decision for the president , his immigration program would immediately take effect , changing the lives of eligible Filipino families and other immigrants .\n",
      "Pred: 0 Ac: 1 Text: Government support to bring the Housing First programme to Wellington will make a real difference for the homeless , says National 's Wellington Central candidate Nicola Willis .\n"
     ]
    }
   ],
   "source": [
    "# Title\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Example dataset (X: text data, y: sentiment labels)\n",
    "X = train_data_text_list\n",
    "y = train_data_label_list  # 1: Positive, 0: Negative (Binary sentiment labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create BoW vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict sentiment labels for the test set\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "calc_scores(y_test, y_pred)\n",
    "\n",
    "# Unseen data\n",
    "unseen_data = test_data_text_list\n",
    "true_labels = test_data_label_list\n",
    "\n",
    "# Vectorize the unseen test data using the same CountVectorizer instance\n",
    "X_unseen_bow = vectorizer.transform(unseen_data)\n",
    "\n",
    "# Predict sentiment labels for the unseen test data\n",
    "y_unseen_pred = classifier.predict(X_unseen_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "calc_scores_and_print_miscalculations(true_labels, y_unseen_pred, unseen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model - SVM - TF-IDF feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7610062893081762\n",
      "F1 Score: 0.6174496644295302\n",
      "Accuracy: 0.8414517669531996\n",
      "F1 Score: 0.336\n",
      "\n",
      "Misclassifications:\n",
      "Pred: 0 Ac: 1 Text: TurkIt 's heartening to see that measures are being taken in Khyber Pakhtunkhwa ( KP ) to empower women and give them work opportunities . You ! takes a look ...\n",
      "Pred: 0 Ac: 1 Text: \"\"\" We are taking a community that is fairly vulnerable and bringing them to a level where they are empowered . \"\" <h> VICTORIA KIOKO\"\n",
      "Pred: 1 Ac: 0 Text: The video shows her trying to read news that was just received from the Associated Press -- that there are so-called tender age shelters , where immigrant babies and young children are being placed after they 're separated from their parents at the U.S. border .\n",
      "Pred: 1 Ac: 0 Text: Having gone through a lot as parents , one thing that the foundation seeks to champion is for the government to establish a hospice facility to lessen the burden on women who normally take care of family members whose medical conditions have been declared hopeless .\n",
      "Pred: 1 Ac: 0 Text: Teachers encourage these eager students to express their feelings in drawings and to share their thoughts in writing . We want them to realize that life does not stop when we encounter struggle , that we can find ways to move forward . We are helping them find solutions to seemingly overwhelming problems instead of resigning themselves to hopelessness .\n",
      "Pred: 0 Ac: 1 Text: \"There were also many larger events which took place , including a 24-hour long period of Eucharistic adoration and a prayer vigil . Additionally , \"\" jubilees \"\" were held which centered on , among others , the sick and disabled , catechists , teenagers , deacons , priests , religious , volunteers of mercy , and most recently , the poor and homeless .\"\n",
      "Pred: 1 Ac: 0 Text: Young people are too often at the sharp end of poverty , either as children of poor families , or as young people trying to live independent lives . Families with children are more likely to live in poverty than others due to the falling level of benefit income , but also due to low wages and increasing costs , given how prices are rising at the moment .\n",
      "Pred: 0 Ac: 1 Text: News this month that Anglican Care spent $4 million buying a Christchurch site as a hub for vulnerable youths is a fine example of that community-minded approach . In years to come there may be an interesting contrast between a beautifully restored cathedral that is empty much of the time and a youth hub and revamped City Mission that are constantly busy .\n",
      "Pred: 1 Ac: 0 Text: A homeless couple is seen by the roadside along Jalan Tuanku Abdul Rahman . -- Picture by Choo Choy May\n",
      "Pred: 1 Ac: 0 Text: They listened to what people said they wanted , paying close attention to the people who would use their machine , and they designed a machine intended to benefit poor families , rural doctors , overloaded nurses , repair technicians .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset (X: text data, y: sentiment labels)\n",
    "X = train_data_text_list\n",
    "y = train_data_label_list # 1: Positive, 0: Negative (Binary sentiment labels)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict sentiment labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "calc_scores(y_test, y_pred)\n",
    "\n",
    "# Example of unseen test data\n",
    "unseen_data = test_data_text_list\n",
    "true_labels = test_data_label_list # True sentiment labels for the unseen test data\n",
    "\n",
    "# Convert the unseen test data into TF-IDF vectors\n",
    "X_unseen_tfidf = tfidf_vectorizer.transform(unseen_data)\n",
    "\n",
    "# Predict sentiment labels for the unseen test data\n",
    "y_unseen_pred = svm_classifier.predict(X_unseen_tfidf)\n",
    "\n",
    "# Calculate accuracy for unseen test data\n",
    "calc_scores_and_print_miscalculations(true_labels, y_unseen_pred, unseen_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       We also know that they can benefit by receivin...\n",
       "1       Pope Francis washed and kissed the feet of Mus...\n",
       "2       Many refugees do n't want to be resettled anyw...\n",
       "3       \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...\n",
       "4       \"In a 90-degree view of his constituency , one...\n",
       "                              ...                        \n",
       "2089    The sad spectacle , which occurred on Saturday...\n",
       "2090    \"\"\" The Pakistani police came to our house and...\n",
       "2091    \"When Marie O'Donoghue went looking for a spec...\n",
       "2092    \"Sri Lankan norms and culture inhibit women fr...\n",
       "2093    He added that the AFP will continue to bank on...\n",
       "Name: text, Length: 2094, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
